# Stream-Processing-using-PySpark-and-MongoDB-
This project simulates a scenario in which a server receives a regular climate data and fire data from a client. The data streams from the client are processes and joined together at the server end using PySpark and then stored in the MongoDB database.

The Client application sets the timing such that one tuple from “Climate Data-Part2” and fivetuple from “Fire Data-Part2” are sent into the stream every second. “DataType” column in the record is used to distinguish between the streams.

The Server application
a. Receives streaming data from the streams generated by the stream server.
b. Processes the streams to ensure the data can be uploaded into the MongoDB.
c. Uploads the result of processing to MongoDB.
